# -*- coding: utf-8 -*-
"""NLP_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T1MjFdxQUs1w_PWFyE51l8P1Sz6Uwjqu
"""

# Uncomment and execute once - this is used to downgrade the pip version to one that doesn't cause an exception in keras
# https://github.com/tensorflow/tensorflow/issues/28102#issuecomment-487612628
!pip install numpy==1.16.2
#!pip install tensorflow-gpu

import numpy as np
import tensorflow as tf
import keras as keras
from keras.models import Sequential, load_model
from keras.layers import Bidirectional, Dense, Dropout, Flatten, SpatialDropout1D
from keras.layers.convolutional import Conv1D, MaxPooling1D
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import GRU, LSTM
from keras.preprocessing import sequence

from google.colab import drive
drive.mount('/content/drive')

IMDB_DATASET_PATH = '/content/drive/My Drive/IDC MSc/Natural Language Processing/Project/imdb_dataset.npz'
MODEL_1_PATH = '/content/drive/My Drive/IDC MSc/Natural Language Processing/Project/model_1.hdf5'
MODEL_2_PATH = '/content/drive/My Drive/IDC MSc/Natural Language Processing/Project/model_2_larger.hdf5'
MODEL_3_PATH = '/content/drive/My Drive/IDC MSc/Natural Language Processing/Project/model_3_larger.hdf5'
MODEL_4_PATH = '/content/drive/My Drive/IDC MSc/Natural Language Processing/Project/model_4.hdf5'
MODEL_5_PATH = '/content/drive/My Drive/IDC MSc/Natural Language Processing/Project/model_5_larger.hdf5'

MAX_LEN = 1000
DATASET_SIZE = 50000

class Model_1_Consts():
    VOCABULARY_SIZE = DATASET_SIZE
    EPOCHS_COUNT = 1
    BATCH_SIZE = 32
    
class Model_2_Consts():
    #VOCABULARY_SIZE = 10000
    VOCABULARY_SIZE = DATASET_SIZE
    EPOCHS_COUNT = 5
    BATCH_SIZE = 64

class Model_3_Consts():
    #VOCABULARY_SIZE = 10000
    VOCABULARY_SIZE = DATASET_SIZE
    EPOCHS_COUNT = 5
    BATCH_SIZE = 64
    
class Model_4_Consts():
    VOCABULARY_SIZE = DATASET_SIZE
    EPOCHS_COUNT = 1
    BATCH_SIZE = 128
    
class Model_5_Consts():
    #VOCABULARY_SIZE = 10000
    VOCABULARY_SIZE = DATASET_SIZE
    EPOCHS_COUNT = 1
    BATCH_SIZE = 128
    
def predict_text_label(text, model):
    words = text.split()
    word_index = keras.datasets.imdb.get_word_index()
    my_index = []
    for word in words:
        if word in word_index:
            my_index.append(word_index[word])
    seq = np.array(my_index)
    padded_seq = keras.preprocessing.sequence.pad_sequences([seq], maxlen=MAX_LEN, dtype='int32', padding='pre')
    return model.predict_classes(padded_seq)

def predict_text(text, model):
    words = text.split()
    word_index = keras.datasets.imdb.get_word_index()
    my_index = []
    for word in words:
        if word in word_index:
            my_index.append(word_index[word])
    seq = np.array(my_index)
    padded_seq = keras.preprocessing.sequence.pad_sequences([seq], maxlen=MAX_LEN, dtype='int32', padding='pre')
    return model.predict_proba(padded_seq)

def build_model_1():
    print("Adding components to model")
    model = Sequential()
    model.add(Embedding(Model_1_Consts.VOCABULARY_SIZE, 32, input_length=MAX_LEN))
    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(250, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    print("Compiling model")
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return model

def build_model_2():
    print("Adding components to model")
    model = Sequential()
    model.add(Embedding(Model_2_Consts.VOCABULARY_SIZE, 32, input_length=MAX_LEN))
    model.add(SpatialDropout1D(0.3))
    model.add(Conv1D(activation="relu", padding="same", filters=64, kernel_size=5))
    model.add(MaxPooling1D(pool_size=4))
    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1, activation='sigmoid'))

    print("Compiling model")
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return model

def build_model_3():
    print("Adding components to model")
    model = Sequential()
    model.add(Embedding(Model_3_Consts.VOCABULARY_SIZE, 32, input_length=MAX_LEN))
    model.add(Dropout(0.2))
    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(GRU(100))
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))
    
    print("Compiling model")
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return model

def build_model_4():
    print("Adding components to model")
    model = Sequential()
    model.add(Embedding(Model_4_Consts.VOCABULARY_SIZE, 32, input_length=MAX_LEN))
    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(MAX_LEN,), merge_mode='sum'))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
	
    print("Compiling model")
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    
    return model

def build_model_5():
    print("Adding components to model")
    model = Sequential()
    model.add(Embedding(Model_5_Consts.VOCABULARY_SIZE, 64, input_length=MAX_LEN))
    model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(MAX_LEN,), merge_mode='concat'))
    model.add(Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
	
    print("Compiling model")
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    
    return model

def train_model(model, epochs_count, batch_size):
    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs_count, batch_size=batch_size, verbose=2)

print("Loading dataset")
(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(path=IMDB_DATASET_PATH, num_words=DATASET_SIZE, seed=1)

x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)
x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)

model_1 = build_model_1()
print("Training model_1")
train_model(model_1, Model_1_Consts.EPOCHS_COUNT, Model_1_Consts.BATCH_SIZE)
print("Done training model_1")

print("Evaluating model_1")
print(model_1.evaluate(x_test, y_test))

print("Saving model_1 to file in path: {}".format(MODEL_1_PATH))
model_1.save(MODEL_1_PATH)
print("Done")

model_2 = build_model_2()
print("Training model_2")
train_model(model_2, Model_2_Consts.EPOCHS_COUNT, Model_2_Consts.BATCH_SIZE)
print("Done training model_2")

print("Evaluating model_2")
print(model_2.evaluate(x_test, y_test))

print("Saving model_2 to file in path: {}".format(MODEL_2_PATH))
model_2.save(MODEL_2_PATH)
print("Done")

model_3 = build_model_3()
print("Training model_3")
train_model(model_3, Model_3_Consts.EPOCHS_COUNT, Model_3_Consts.BATCH_SIZE)
print("Done training model_3")

print("Evaluating model_3")
print(model_3.evaluate(x_test, y_test))

print("Saving model_3 to file in path: {}".format(MODEL_3_PATH))
model_3.save(MODEL_3_PATH)
print("Done")

model_4 = build_model_4()
print("Training model_4")
train_model(model_4, Model_4_Consts.EPOCHS_COUNT, Model_4_Consts.BATCH_SIZE)
print("Done training model_4")

print("Evaluating model_4")
print(model_4.evaluate(x_test, y_test))

print("Saving model_4 to file in path: {}".format(MODEL_4_PATH))
model_4.save(MODEL_4_PATH)
print("Done")

model_5 = build_model_5()
print("Training model_5")
train_model(model_5, Model_5_Consts.EPOCHS_COUNT, Model_5_Consts.BATCH_SIZE)
print("Done training model_5")

print("Evaluating model_5")
print(model_5.evaluate(x_test, y_test))

print("Saving model_5 to file in path: {}".format(MODEL_5_PATH))
model_5.save(MODEL_5_PATH)
print("Done")
